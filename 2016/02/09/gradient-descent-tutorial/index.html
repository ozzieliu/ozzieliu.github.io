<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Python Tutorial on Linear Regression with Batch Gradient Descent - The Land of Oz</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="The Land of Oz" property="og:site_name">
  
    <meta content="Python Tutorial on Linear Regression with Batch Gradient Descent" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="Journey to Data Science
" property="og:description">
  
  
    <meta content="http://localhost:4000/2016/02/09/gradient-descent-tutorial/" property="og:url">
  
  
    <meta content="2016-02-09T23:12:00-05:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/assets/img/gradientdescent.png" property="og:image">
  
  
    
  
  
    
    <meta content="data-science" property="article:tag">
    
    <meta content="python" property="article:tag">
    
    <meta content="tutorial" property="article:tag">
    
    <meta content="machine-learning" property="article:tag">
    
  

  
    <meta name="twitter:card" content="">
  
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@ozzieliu">
  
    <meta name="twitter:title" content="Python Tutorial on Linear Regression with Batch Gradient Descent">
  
  
    <meta name="twitter:url" content="http://localhost:4000/2016/02/09/gradient-descent-tutorial/">
  
  
    <meta name="twitter:description" content="Journey to Data Science
">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/gradientdescent.png">
  

	<meta name="description" content="">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicon/apple-touch-icon-114x114.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/profile.png" alt="Ozzie Liu"></a>
      </div>
      <div class="author-name">Ozzie Liu</div>

      <p>The Land of Oz</p>

      <p class="about-quote">"...to embark upon a hazardous and technically unexplainable journey into the outer stratosphere" of data science.</p>

      <h3 class="contact-title">.</h3>

      <p><a href="/about">About Ozzie</a></p>
      <p><a href="/projects">Projects</a></p>
      <p><a href="/contact">Contact me</a></p>
    </div>
  </header> <!-- End Header -->

  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
          <li><a href="https://twitter.com/ozzieliu" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        <!-- 
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
         -->
        
          <li class="github"><a href="http://github.com/ozzieliu" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://www.linkedin.com/in/ozzieliu" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li class="email"><a href="/contact"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>version 1.01 (hot air balloon)<br>
      2019 &copy; Ozzie Liu</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <img class="page-image" src=/assets/img/gradientdescent.png alt="Python Tutorial on Linear Regression with Batch Gradient Descent">
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Python Tutorial on Linear Regression with Batch Gradient Descent</h1>
        <div class="page-date"><span>Feb 09, 2016&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>I learn best by doing and teaching. And while Python has some excellent packages available for linear regression (like Statsmodels or Scikit-learn), I wanted to understand the intuition behind ordinary least squares (OLS) linear regression. How is the best fit found? How do you actually implement batch gradient descent? <!--more--> There are plenty of great explanation on gradient descent here, here and here, so my goal here is provide some insight as I implement it with Python.</p>

<p>This guide is primarily modeled after Andrew Ng’s excellent <a href="http://www.ml-class.org/" target="_blank">Machine Learning course</a> available online for free. The programming exercises there are straight forward and intuitive, but it’s mainly in Matlab/Octave. I wanted to implement the same thing in Python with Numpy arrays.</p>

<p>You can also find the iPython Notebook version of this tutorial available on my <a href="https://github.com/ozzieliu/python-tutorials/tree/master/Linear%20Regression" target="_blank">Github</a>,
or in HTML format <a href="http://www.ozzieliu.com/tutorials/Linear-Regression-Gradient-Descent.html" target="_blank">here</a></p>

<h3 id="contents">Contents:</h3>
<ul>
  <li><a href="#loading-the-data">Loading the data</a></li>
  <li><a href="#plotting-the-data">Plotting the data</a></li>
  <li><a href="#cost-function">Cost function</a></li>
  <li><a href="#gradient-descent">Gradient descent</a></li>
  <li><a href="#python-implementation">Python implementation</a></li>
  <li><a href="#interpretation">Interpretation</a></li>
  <li><a href="#prediction">Prediction</a></li>
</ul>

<h3 id="packages">Packages:</h3>
<ul>
  <li><code class="highlighter-rouge">Pandas</code> for data frames and easy to read csv files</li>
  <li><code class="highlighter-rouge">Numpy</code> for array and matrix mathematics functions</li>
  <li><code class="highlighter-rouge">Matplotlib</code> for plotting</li>
</ul>

<h2 id="loading-the-data">Loading the Data</h2>

<p>Let’s start by performing a linear regression with one variable to predict profits for a food truck. The data contains 2 columns, population of a city (in 10,000s) and the profits of the food truck (in 10,000s).</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'ex1data1.txt'</span><span class="p">,</span> <span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'population'</span><span class="p">,</span> <span class="s">'profit'</span><span class="p">])</span>
<span class="k">print</span> <span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#  population   profit
0      6.1101  17.5920
1      5.5277   9.1302
2      8.5186  13.6620
3      7.0032  11.8540
4      5.8598   6.8233
</code></pre></div></div>

<h2 id="plotting-the-data">Plotting the Data</h2>

<p><img src="/assets/img/LR-data.png" alt="plotting the data" /></p>

<p>So this is what our data points look like when plotted out.</p>

<p>The idea of linear regression is to find a relationship between our target or dependent variable (y) and a set of explanatory variables (\(x_1, x_2…\)). This relationship can then be used to predict other values.</p>

<p>In our case with one variable, this relationship is a line defined by parameters \(\beta\) and the following form: \(y = \beta_0 + \beta_1x\), where \(\beta_0\) is our intercept. (I chose to use \(\beta\) but many literature uses \(Theta\), so keep that in mind)</p>

<p>This can be extended to multivariable regression by extending the equation in vector form: \(y=X\beta\)</p>

<p>So how do I make the best line? In this figure, there are many possible lines. Which one is the best?</p>

<h2 id="cost-function">Cost Function</h2>
<p>It turns out that to make the best line to model the data, we want to pick parameters \(\beta\) that allows our predicted value to be as close to the actual value as possible. In other words, we want the distance or residual between our hypothesis \(h(x)\) and y to be minimized.</p>

<p>So we formally define a cost function using ordinary least squares that is simply the sum of the squared distances. To find the liner regression line, we adjust our beta parameters to minimize:</p>

<script type="math/tex; mode=display">J(\beta) = \frac{1}{2m}\sum_{i=1}^m(h_\beta(x^{(i)})-y^{(i)})^2</script>

<p>Again the hypothesis that we’re trying to find is given by the linear model:</p>

<script type="math/tex; mode=display">h_\beta(x) = \beta^{T}x = \beta_0 + \beta_1x_1</script>

<p>And we can use batch gradient descent where each iteration performs the update</p>

<script type="math/tex; mode=display">\beta_j := \beta_j - \alpha\frac{1}{m}\sum_{i=1}^m (h_\beta(x^{(i)})-y^{(i)})x_{j}^{(i)}</script>

<h2 id="gradient-descent">Gradient Descent?</h2>

<p>Whoa, what’s gradient descent? And why are we updating that?</p>

<p>Gradient descent simply is an algorithm that makes small steps along a function to find a local minimum. We can look at a simply quadratic equation such as this one:</p>

<p><img src="/assets/img/LR-quadratic.png" alt="quadratic" /></p>

<p>We’re trying to find the local minimum on this function. If we start at the first red dot at x = 2, we find the gradient and we move against it. In this case, the gradient is the slope. And since the slope is negative, our next attempt is further to the right. Thus bringing us closer to the minimum.</p>

<p>If we start at the right-most blue dot at x = 8, our gradient or slope is positive, so we move away from that by multiplying it by a -1. And it eventually makes smaller and smaller updates (as the gradient approaches 0 at the minimum) until the parameter converges at the minimum we’re looking for.</p>

<p>Indeed, we keep updating our parameter beta to get us closer and closer to the minimum.</p>

<script type="math/tex; mode=display">\beta_j := \beta_j - \alpha\frac{\partial}{\partial \beta_j} J(\beta)</script>

<p>Where \(\alpha\) is our learning rate and we find the partial differentiation of our cost function in respect to beta. I won’t derive the partial differentiation today, but it results in the formula right before this.</p>

<p>By adjusting alpha, we can change how quickly we converge to the minimum (at the risk of overshooting it entirely and does not converge on our local minimum)</p>

<p>That’s in a 2 dimensional space. In a 3D space, it would be like rolling a ball down a hill to find the lowest point. I can’t picture anything above 3 dimensions, but that’s the idea.</p>

<p>This method is called <strong>“batch” gradient descent</strong> because we use the entire <strong>batch</strong> of points X to calculate each gradient, as opposed to stochastic gradient descent. which uses one point at a time. I’ll implement stochastic gradient descent in a future tutorial.</p>

<h2 id="python-implementation">Python Implementation</h2>

<p>OK, let’s try to implement this in Python. First I declare some parameters. Alpha is my learning rate, and iterations defines how many times I want to perform the update.</p>

<p>Then I transform the data frame holding my data into an array for simpler matrix math. And then write a function to calculate the cost function as defined above. I use <code class="highlighter-rouge">np.dot</code> for inner matrix multiplication. Here’s what it looks like:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">iterations</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="c">## Add a columns of 1s as intercept to X. This becomes the 2nd column</span>
<span class="n">X_df</span><span class="p">[</span><span class="s">'intercept'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c">## Transform to Numpy arrays for easier matrix math</span>
<span class="c">## and start beta at 0, 0</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_df</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_df</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="s">"""
    cost_function(X, y, beta) computes the cost of using beta as the
    parameter for linear regression to fit the data points in X and y
    """</span>
    <span class="c">## number of training examples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c">## Calculate the cost with the given parameters</span>
    <span class="n">J</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="o">/</span><span class="n">m</span>

    <span class="k">return</span> <span class="n">J</span>

<span class="n">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">32.07</span>
</code></pre></div></div>

<p>Now, let’s implement gradient descent. Remember that long equation above?
<script type="math/tex">\beta_j := \beta_j - \alpha\frac{1}{m}\sum_{i=1}^m (h_\beta(x^{(i)})-y^{(i)})x_{j}^{(i)}</script></p>

<p>I’m going to split them into separate parts so that I can see what’s going on.
Plus, I like to check my matrix dimensions to make sure that I’m doing the math in the right order.
Oh and since I’m doing the math matrices now, I can exclude the summation of i terms.</p>

<ol>
  <li>Calculate <strong>hypothesis</strong> \(h_\beta(x)\):
    <ul>
      <li>hypothesis [97x1] = x [97x2] * beta [2x1]</li>
    </ul>
  </li>
  <li>Calculate <strong>loss</strong> \((h_\beta(x)-y)\):
    <ul>
      <li>loss [97x1] with element-wise subtraction</li>
    </ul>
  </li>
  <li>Calculate <strong>gradient</strong> \((h_\beta(x)-y)x_{j}\):
    <ul>
      <li>gradient [2x1] = X’ [2x97] * loss [97x1]</li>
    </ul>
  </li>
  <li>Update parameter <strong>beta</strong>:
    <ul>
      <li>[2x1] after element-wise subtraction multiplied by a scalar</li>
    </ul>
  </li>
  <li>And find the <strong>cost</strong> by using <code class="highlighter-rouge">cost_function()</code></li>
</ol>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">):</span>
    <span class="s">"""
    gradient_descent() performs gradient descent to learn beta by
    taking num_iters gradient steps with learning rate alpha
    """</span>
    <span class="n">cost_history</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">iterations</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">hypothesis</span><span class="o">-</span><span class="n">y</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">*</span><span class="n">gradient</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        <span class="n">cost_history</span><span class="p">[</span><span class="n">iteration</span><span class="p">]</span> <span class="o">=</span> <span class="n">cost</span>

        <span class="c">## If you really want to merge everything in one line:</span>
        <span class="c"># beta = beta - alpha * (X.T.dot(X.dot(beta)-y)/m)</span>
        <span class="c"># cost = cost_function(X, y, beta)</span>
        <span class="c"># cost_history[iteration] = cost</span>

    <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">cost_history</span>

<span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">)</span>

<span class="k">print</span> <span class="n">b</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">[</span> <span class="mf">1.16636235</span> <span class="o">-</span><span class="mf">3.63029144</span><span class="p">]</span>
</code></pre></div></div>

<h2 id="interpretation">Interpretation</h2>

<p>How can we interpret the beta parameters? Well \(\beta_0\) is the intercept of
the line in 2D. In our case, since we added the intercept column of 1s afterwards,
it is actually the second column of the matrix X. So the corresponding beta is the
second one. Our intercept is -3.6. Indeed, we can see on the graph with the best
fit line that this is true. \(\beta_0\) is then the slope of the line.</p>

<p>With everything put together:</p>

<p><img src="/assets/img/LR-LR.png" alt="linear-regression" /></p>

<h2 id="prediction">Prediction</h2>

<p>Now We can use our trained linear regression model to predict profits in cities
of certain sizes. Let’s try 2 cities, with population of 35,000 and 70,000. Since
I have my parameters defined, I can plug them in to the linear regression model:
<script type="math/tex">\hat{y} = -3.603 + 1.166x</script></p>

<p>or make them a matrix x and multiple them by beta</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="k">print</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">0.45197678677</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">4.53424501294</span>
</code></pre></div></div>

<p>Profits are about $4,519 and $45,342 respectively.</p>

<p>See, gradient descent isn’t difficult to understand anymore. Of course, I glossed
over some important considerations (such as learning rate and number of iterations
until converging on a minumum), and they may be topics for another day, but this
is the general concept.</p>

<p>Next up, we’ll take a look at regularization and multi-variable regression, before
exploring logistic regression and other supervised learning algorithms.</p>

<p>Let me know if this was helpful or if you spotted any errors.</p>

      <div class="page-footer">
        <!-- <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=Python Tutorial on Linear Regression with Batch Gradient Descent&url=http://localhost:4000/2016/02/09/gradient-descent-tutorial/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/2016/02/09/gradient-descent-tutorial/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=http://localhost:4000/2016/02/09/gradient-descent-tutorial/" title="Share on Google+" rel="nofollow" target="_blank">Google+</a>
        </div> -->
        <div class="page-tag">
          
            <a href="/projects/#data-science" class="tag">&#35; data-science</a>
          
            <a href="/projects/#python" class="tag">&#35; python</a>
          
            <a href="/projects/#tutorial" class="tag">&#35; tutorial</a>
          
            <a href="/projects/#machine-learning" class="tag">&#35; machine-learning</a>
          
        </div>
      </div>
      <!-- <section class="comment-area">
  <div class="comment-wrapper">
    
  </div>
</section> <!-- End Comment Area -->
 -->
    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-61700249-1', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
